{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d0013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ETL Load Phase for Lab 5...\n",
      "Setup complete. File paths and database connections established.\n",
      "Table full_data created successfully.\n",
      "Saved DataFrame to loaded_data\\full_data.parquet\n",
      "Full data loaded successfully into SQLite and Parquet.\n",
      "Table incremental_data created successfully.\n",
      "Saved DataFrame to loaded_data\\incremental_data.parquet\n",
      "Incremental data loaded successfully into SQLite and Parquet.\n",
      "\n",
      "Verifying SQLite full_data table:\n",
      "Preview of full_data:\n",
      "   id customer_name   product  quantity  unit_price  total_price  \\\n",
      "0   1       Unknown  Clothing         1          53           53   \n",
      "1   2       Unknown  Clothing         1          64           64   \n",
      "2   3       Unknown  Clothing         1          73           73   \n",
      "3   4       Unknown  Footwear         1          90           90   \n",
      "4   5       Unknown  Clothing         1          49           49   \n",
      "\n",
      "            order_date  \n",
      "0  2025-04-20 12:00:00  \n",
      "1  2025-04-20 12:00:00  \n",
      "2  2025-04-21 12:00:00  \n",
      "3  2025-04-22 12:00:00  \n",
      "4  2025-04-23 12:00:00  \n",
      "\n",
      "\n",
      "Verifying SQLite incremental_data table:\n",
      "Preview of incremental_data:\n",
      "     id customer_name      product  quantity  unit_price  total_price  \\\n",
      "0  1000       Unknown     Clothing         1          28           28   \n",
      "1  1001       Unknown     Clothing         1          46           46   \n",
      "2  1002       Unknown     Clothing         1          60           60   \n",
      "3  1003       Unknown     Footwear         1          59           59   \n",
      "4  1004       Unknown  Accessories         1          68           68   \n",
      "\n",
      "   order_date  \n",
      "0  2028-01-13  \n",
      "1  2028-01-14  \n",
      "2  2028-01-15  \n",
      "3  2028-01-16  \n",
      "4  2028-01-17  \n",
      "\n",
      "\n",
      "Verifying full_data.parquet:\n",
      "Preview of loaded_data\\full_data.parquet:\n",
      "   id customer_name   product  quantity  unit_price  total_price  \\\n",
      "0   1       Unknown  Clothing         1          53           53   \n",
      "1   2       Unknown  Clothing         1          64           64   \n",
      "2   3       Unknown  Clothing         1          73           73   \n",
      "3   4       Unknown  Footwear         1          90           90   \n",
      "4   5       Unknown  Clothing         1          49           49   \n",
      "\n",
      "            order_date  \n",
      "0  2025-04-20 12:00:00  \n",
      "1  2025-04-20 12:00:00  \n",
      "2  2025-04-21 12:00:00  \n",
      "3  2025-04-22 12:00:00  \n",
      "4  2025-04-23 12:00:00  \n",
      "\n",
      "\n",
      "Verifying incremental_data.parquet:\n",
      "Preview of loaded_data\\incremental_data.parquet:\n",
      "     id customer_name      product  quantity  unit_price  total_price  \\\n",
      "0  1000       Unknown     Clothing         1          28           28   \n",
      "1  1001       Unknown     Clothing         1          46           46   \n",
      "2  1002       Unknown     Clothing         1          60           60   \n",
      "3  1003       Unknown     Footwear         1          59           59   \n",
      "4  1004       Unknown  Accessories         1          68           68   \n",
      "\n",
      "   order_date  \n",
      "0  2028-01-13  \n",
      "1  2028-01-14  \n",
      "2  2028-01-15  \n",
      "3  2028-01-16  \n",
      "4  2028-01-17  \n",
      "\n",
      "ETL Load Phase for Lab 5 completed.\n"
     ]
    }
   ],
   "source": [
    "# etl_load.py\n",
    "# DSA 2040A - Lab 4: Load in ETL (Shopping Trends)\n",
    "# Name: Queen Esther Kibegi\n",
    "# Course: DSA 2040A – Data Science & Analytics\n",
    "# Lab: Lab 4 – Load in ETL\n",
    "# Dataset Used: transformed_full.csv, transformed_incremental.csv\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sqlite_table(conn, table_name, schema):\n",
    "    \"\"\"\n",
    "    Create a SQLite table with the given schema.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection object\n",
    "        table_name: Name of the table to create\n",
    "        schema: SQL schema string for table creation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(schema)\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} created successfully.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error creating table {table_name}: {e}\")\n",
    "\n",
    "def save_to_parquet(df, output_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to save\n",
    "        output_path: Path to save the Parquet file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        print(f\"Saved DataFrame to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Parquet: {e}\")\n",
    "\n",
    "def verify_sqlite_data(conn, table_name):\n",
    "    \"\"\"\n",
    "    Verify data in a SQLite table by printing the first 5 rows.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection object\n",
    "        table_name: Name of the table to query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"Preview of {table_name}:\\n{df}\\n\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error verifying {table_name}: {e}\")\n",
    "\n",
    "def verify_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Verify data in a Parquet file by printing the first 5 rows.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Parquet file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"Preview of {file_path}:\\n{df.head()}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying {file_path}: {e}\")\n",
    "\n",
    "def validate_dates(df, date_column):\n",
    "    \"\"\"\n",
    "    Validate and convert date column to datetime, handling invalid dates.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        date_column: Name of the date column to validate\n",
    "    Returns:\n",
    "        DataFrame with validated date column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "        invalid_dates = df[df[date_column].isna()]\n",
    "        if not invalid_dates.empty:\n",
    "            print(f\"Warning: {len(invalid_dates)} rows with invalid dates in {date_column} set to NaT.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating dates in {date_column}: {e}\")\n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    print(\"Starting ETL Load Phase for Lab 5...\")\n",
    "\n",
    "    # Section 1: Load Setup\n",
    "    # Define file paths\n",
    "    FULL_CSV_PATH = 'transformed_full.csv'\n",
    "    INCREMENTAL_CSV_PATH = 'transformed_incremental.csv'\n",
    "    OUTPUT_DIR = 'loaded_data'\n",
    "    FULL_DB_PATH = os.path.join(OUTPUT_DIR, 'full_data.db')\n",
    "    INCREMENTAL_DB_PATH = os.path.join(OUTPUT_DIR, 'incremental_data.db')\n",
    "    FULL_PARQUET_PATH = os.path.join(OUTPUT_DIR, 'full_data.parquet')\n",
    "    INCREMENTAL_PARQUET_PATH = os.path.join(OUTPUT_DIR, 'incremental_data.parquet')\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # SQLite connection setup\n",
    "    full_conn = sqlite3.connect(FULL_DB_PATH)\n",
    "    incremental_conn = sqlite3.connect(INCREMENTAL_DB_PATH)\n",
    "    full_engine = create_engine(f'sqlite:///{FULL_DB_PATH}')\n",
    "    incremental_engine = create_engine(f'sqlite:///{INCREMENTAL_DB_PATH}')\n",
    "\n",
    "    print('Setup complete. File paths and database connections established.')\n",
    "\n",
    "    # Section 2: Load Full Transformed Data\n",
    "    FULL_TABLE_SCHEMA = '''\n",
    "    CREATE TABLE IF NOT EXISTS full_data (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        customer_name TEXT,\n",
    "        product TEXT,\n",
    "        quantity INTEGER,\n",
    "        unit_price REAL,\n",
    "        total_price REAL,\n",
    "        order_date TEXT\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Read CSV\n",
    "        full_df = pd.read_csv(FULL_CSV_PATH)\n",
    "        \n",
    "        # Validate Order Date\n",
    "        full_df = validate_dates(full_df, 'Order Date')\n",
    "        \n",
    "        # Map dataset columns to schema\n",
    "        mapped_df = pd.DataFrame({\n",
    "            'id': full_df.get('Customer ID', pd.Series(range(1, len(full_df) + 1))),\n",
    "            'customer_name': pd.Series('Unknown', index=full_df.index),  # No customer_name in dataset\n",
    "            'product': full_df.get('Category', pd.Series('Unknown', index=full_df.index)),\n",
    "            'quantity': 1,  # Assumed as 1 per dataset structure\n",
    "            'unit_price': full_df.get('Purchase Amount (USD)', 0.0),\n",
    "            'total_price': full_df.get('Purchase Amount (USD)', 0.0),  # No Total_Price, use Purchase Amount\n",
    "            'order_date': full_df.get('Order Date', pd.Series(None, index=full_df.index)).astype(str)\n",
    "        })\n",
    "        \n",
    "        # Create SQLite table\n",
    "        create_sqlite_table(full_conn, 'full_data', FULL_TABLE_SCHEMA)\n",
    "        \n",
    "        # Load to SQLite\n",
    "        mapped_df.to_sql('full_data', full_engine, if_exists='replace', index=False)\n",
    "        \n",
    "        # Save to Parquet\n",
    "        save_to_parquet(mapped_df, FULL_PARQUET_PATH)\n",
    "        \n",
    "        print('Full data loaded successfully into SQLite and Parquet.')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading full data: {e}')\n",
    "\n",
    "    # Section 3: Load Incremental Transformed Data\n",
    "    INCREMENTAL_TABLE_SCHEMA = '''\n",
    "    CREATE TABLE IF NOT EXISTS incremental_data (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        customer_name TEXT,\n",
    "        product TEXT,\n",
    "        quantity INTEGER,\n",
    "        unit_price REAL,\n",
    "        total_price REAL,\n",
    "        order_date TEXT\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Read CSV\n",
    "        incremental_df = pd.read_csv(INCREMENTAL_CSV_PATH)\n",
    "        \n",
    "        # Validate Order Date\n",
    "        incremental_df = validate_dates(incremental_df, 'Order Date')\n",
    "        \n",
    "        # Map dataset columns to schema\n",
    "        mapped_incremental_df = pd.DataFrame({\n",
    "            'id': incremental_df.get('Customer ID', pd.Series(range(1, len(incremental_df) + 1))),\n",
    "            'customer_name': pd.Series('Unknown', index=incremental_df.index),  # No customer_name in dataset\n",
    "            'product': incremental_df.get('Category', pd.Series('Unknown', index=incremental_df.index)),\n",
    "            'quantity': 1,  # Assumed as 1 per dataset structure\n",
    "            'unit_price': incremental_df.get('Purchase Amount (USD)', 0.0),\n",
    "            'total_price': incremental_df.get('Purchase Amount (USD)', 0.0),  # No Total_Price, use Purchase Amount\n",
    "            'order_date': incremental_df.get('Order Date', pd.Series(None, index=incremental_df.index)).astype(str)\n",
    "        })\n",
    "        \n",
    "        # Create SQLite table\n",
    "        create_sqlite_table(incremental_conn, 'incremental_data', INCREMENTAL_TABLE_SCHEMA)\n",
    "        \n",
    "        # Load to SQLite\n",
    "        mapped_incremental_df.to_sql('incremental_data', incremental_engine, if_exists='replace', index=False)\n",
    "        \n",
    "        # Save to Parquet\n",
    "        save_to_parquet(mapped_incremental_df, INCREMENTAL_PARQUET_PATH)\n",
    "        \n",
    "        print('Incremental data loaded successfully into SQLite and Parquet.')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading incremental data: {e}')\n",
    "\n",
    "    # Section 4: Verification\n",
    "    print('\\nVerifying SQLite full_data table:')\n",
    "    verify_sqlite_data(full_conn, 'full_data')\n",
    "\n",
    "    print('\\nVerifying SQLite incremental_data table:')\n",
    "    verify_sqlite_data(incremental_conn, 'incremental_data')\n",
    "\n",
    "    print('\\nVerifying full_data.parquet:')\n",
    "    verify_parquet_data(FULL_PARQUET_PATH)\n",
    "\n",
    "    print('\\nVerifying incremental_data.parquet:')\n",
    "    verify_parquet_data(INCREMENTAL_PARQUET_PATH)\n",
    "\n",
    "    # Close database connections\n",
    "    full_conn.close()\n",
    "    incremental_conn.close()\n",
    "    print(\"ETL Load Phase for Lab 5 completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
